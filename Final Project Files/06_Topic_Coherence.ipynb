{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba95031-404c-4dc6-b419-e9756b6a3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import logging\n",
    "from charset_normalizer import from_path\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the spacy model once\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def read_screenplay(file_path):\n",
    "    try:\n",
    "        result = from_path(file_path).best()\n",
    "        with open(file_path, 'r', encoding=result.encoding) as file:\n",
    "            lines = file.readlines()\n",
    "        logging.info(f\"Successfully read file {file_path}\")\n",
    "        return ''.join(lines[1:])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def separate_scenes(text):\n",
    "    try:\n",
    "        scenes = []\n",
    "        raw_scenes = [scene.strip() for scene in text.split('=' * 50) if scene.strip()]\n",
    "        for raw_scene in raw_scenes:\n",
    "            scene_lines = raw_scene.split('\\n')\n",
    "            scene_text = '\\n'.join(scene_lines[1:]).strip()\n",
    "            scenes.append(scene_text)\n",
    "        logging.info(f\"Separated text into {len(scenes)} scenes\")\n",
    "        return scenes\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error separating scenes: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def merge_short_scenes(scenes, min_words=100):\n",
    "    merged_scenes = []\n",
    "    current_scene = \"\"\n",
    "    for scene in scenes:\n",
    "        current_scene_word_count = len(current_scene.split())\n",
    "        scene_word_count = len(scene.split())\n",
    "        if current_scene_word_count + scene_word_count < min_words:\n",
    "            current_scene += \" \" + scene\n",
    "        else:\n",
    "            if current_scene:\n",
    "                merged_scenes.append(current_scene.strip())\n",
    "            current_scene = scene\n",
    "    if current_scene:\n",
    "        merged_scenes.append(current_scene.strip())\n",
    "    logging.info(f\"Merged scenes into {len(merged_scenes)} longer scenes\")\n",
    "    return merged_scenes\n",
    "\n",
    "def identify_character_names(text):\n",
    "    character_name_pattern = re.compile(r'\\n\\s*([A-Z][A-Z\\s]+)\\s*\\n')\n",
    "    potential_characters = character_name_pattern.findall(text)\n",
    "    cleaned_characters = [re.sub(r'\\s+$', '', char) for char in potential_characters]\n",
    "    return cleaned_characters\n",
    "\n",
    "def preprocess_text(text):\n",
    "    character_names = identify_character_names(text)\n",
    "    for name in character_names:\n",
    "        text = text.replace(name, '')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in STOPWORDS] for doc in texts]\n",
    "\n",
    "def train_lda_model(texts, num_topics=10, passes=10):\n",
    "    dictionary = Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "    return lda_model, dictionary, corpus\n",
    "\n",
    "def get_dominant_topic(lda_model, text, dictionary):\n",
    "    bow = dictionary.doc2bow(text)\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "def assign_dominant_topics(texts, lda_model, dictionary):\n",
    "    dominant_topics = [get_dominant_topic(lda_model, text, dictionary) for text in texts]\n",
    "    return dominant_topics\n",
    "\n",
    "def compute_topic_coherence(dominant_topics, lda_model):\n",
    "    topic_vectors = lda_model.get_topics()\n",
    "    topic_vectors = topic_vectors / np.linalg.norm(topic_vectors, axis=1, keepdims=True)  # Normalize vectors\n",
    "    \n",
    "    # Get vectors for the dominant topics\n",
    "    scene_vectors = [topic_vectors[topic] for topic in dominant_topics]\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    similarities = cosine_similarity(scene_vectors)\n",
    "    \n",
    "    # Compute the average similarity (excluding self-similarity)\n",
    "    num_scenes = len(scene_vectors)\n",
    "    sum_similarities = np.sum(similarities) - num_scenes  # Subtract diagonal (self-similarity)\n",
    "    avg_similarity = sum_similarities / (num_scenes * (num_scenes - 1))\n",
    "    \n",
    "    return avg_similarity\n",
    "\n",
    "def process_screenplay(filename, screenplay_folder):\n",
    "    try:\n",
    "        file_path = os.path.join(screenplay_folder, filename)\n",
    "        pattern = re.compile(r'_0*(\\d+)\\.txt$')\n",
    "        match = pattern.search(filename)\n",
    "        if not match:\n",
    "            logging.warning(f\"Could not extract imdbid from filename: {filename}\")\n",
    "            return None, None\n",
    "\n",
    "        imdbid = match.group(1)\n",
    "        text = read_screenplay(file_path)\n",
    "        if text is None:\n",
    "            return None, None\n",
    "\n",
    "        scenes = separate_scenes(text)\n",
    "        merged_scenes = merge_short_scenes(scenes, min_words=100)\n",
    "        preprocessed_scenes = [preprocess_text(scene) for scene in merged_scenes]\n",
    "        preprocessed_scenes = remove_stopwords(preprocessed_scenes)\n",
    "        lda_model, dictionary, corpus = train_lda_model(preprocessed_scenes, num_topics=10, passes=10)\n",
    "        dominant_topics = assign_dominant_topics(preprocessed_scenes, lda_model, dictionary)\n",
    "        overall_coherence = compute_topic_coherence(dominant_topics, lda_model)\n",
    "        \n",
    "        logging.info(f\"Processed {filename}: imdbid={imdbid}, coherence={overall_coherence}\")\n",
    "        return int(imdbid), overall_coherence\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {filename}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3400730-d314-48e1-a88a-b1351474c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the metadata dataframe\n",
    "        df = pd.read_csv('data/movie_metadata_final.csv')\n",
    "\n",
    "        # Folder containing screenplay files\n",
    "        screenplay_folder = 'data/screenplay_data/data/scene_separated_texts'\n",
    "\n",
    "        # List to store results\n",
    "        results = []\n",
    "\n",
    "        # Iterate over screenplay files sequentially\n",
    "        for filename in os.listdir(screenplay_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                result = process_screenplay(filename, screenplay_folder)\n",
    "                if result[0] is not None and result[1] is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        # Create a new DataFrame to store imdbid and overall_coherence\n",
    "        coherence_df = pd.DataFrame(results, columns=['imdbid', 'overall_coherence'])\n",
    "\n",
    "        # Drop rows where imdbid or overall_coherence is None\n",
    "        coherence_df.dropna(inplace=True)\n",
    "\n",
    "        # Save the results DataFrame\n",
    "        coherence_df.to_csv('data/movie_coherence_scores.csv', index=False)\n",
    "\n",
    "        # Merge the coherence scores back into the original metadata DataFrame\n",
    "        df = df.merge(coherence_df, on='imdbid', how='left')\n",
    "\n",
    "        # Save the updated metadata DataFrame\n",
    "        df.to_csv('data/movie_metadata_with_coherence.csv', index=False)\n",
    "        \n",
    "        logging.info(\"Finished processing all screenplays and saved results.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
